{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f870ba-0ff5-4787-8c3e-c435ace12391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> .jp-Button path { fill: #616161;} text.terms { fill: #616161;} .jp-icon-warn0 path {fill: var(--jp-warn-color0);} .bp3-button-text path { fill: var(--jp-inverse-layout-color3);} .jp-icon-brand0 path { fill: var(--jp-brand-color0);} text.terms { fill: #616161;} </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "css_str = '<style> \\\n",
    ".jp-Button path { fill: #616161;} \\\n",
    "text.terms { fill: #616161;} \\\n",
    ".jp-icon-warn0 path {fill: var(--jp-warn-color0);} \\\n",
    ".bp3-button-text path { fill: var(--jp-inverse-layout-color3);} \\\n",
    ".jp-icon-brand0 path { fill: var(--jp-brand-color0);} \\\n",
    "text.terms { fill: #616161;} \\\n",
    "</style>'\n",
    "display(HTML(css_str ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbd05db-1501-4a66-aec1-4452f9ef4504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pprint import pprint\n",
    "from numpy import random\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#Text-preprocessing (lemmatization) from spacy\n",
    "import spacy\n",
    "\n",
    "#Visualization tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "163bc29c-d0ef-44d9-a281-ecd109d47027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7614793c-ae59-4667-b4f5-5e6d21a33411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
      " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
      " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
      " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
      " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
      " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import data 'Newsgroup'\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b258131e-03af-4824-bd46-2c99a7cf773e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: 15 I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/ early 70s. It was called a Bricklin. The doors were really small. In addition, the front bumper was separate from the rest of the body. This is all I know. If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e-mail. Thanks, - IL ---- brought to you by your neighborhood Lerxst ---- ']\n"
     ]
    }
   ],
   "source": [
    "#Convert to list and remove emails, newline, single quotes\n",
    "data = df.content.values.tolist()\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e721889e-e36b-4dde-b5d2-67df06811e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "#Tokenize words by gensim's simple_preprocess()\n",
    "def tokenization(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "data_words = list(tokenization(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f52f754-f046-443c-849b-25eb055db724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['wheres', 'thing', 'car', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'organization', 'university', 'maryland', 'college', 'park', 'lines', 'wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'door', 'sports', 'car', 'looked', 'late', 'early', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "#Remove stopwords by spacy\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "print(data_words_nostops[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ea3619a-0897-419c-8304-f9bd856d78fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['wheres', 'thing', 'car', 'nntp_posting', 'host', 'rac_wam', 'umd', 'organization', 'university', 'maryland_college', 'park', 'lines', 'wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'door', 'sports', 'car', 'looked', 'late', 'early', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'front_bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "#Create Bigram by gensim's Phrases()\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "print(data_words_bigrams[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ba49e81-9247-4e8c-bf9e-6ff438e3d297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['s', 'thing', 'car', 'nntp_poste', 'host', 'umd', 'organization', 'park', 'line', 'wonder', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization by spacy's en_core_web_sm, only keep noun, adj, vb, adv\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92660b06-1680-433c-ab61-21ff42544da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[((0, 'addition'), 1), ((1, 'body'), 1), ((2, 'bricklin'), 1), ((3, 'bring'), 1), ((4, 'call'), 1), ((5, 'car'), 5), ((6, 'day'), 1), ((7, 'door'), 2), ((8, 'early'), 1), ((9, 'engine'), 1), ((10, 'enlighten'), 1), ((11, 'funky'), 1), ((12, 'history'), 1), ((13, 'host'), 1), ((14, 'info'), 1), ((15, 'know'), 1), ((16, 'late'), 1), ((17, 'lerxst'), 1), ((18, 'line'), 1), ((19, 'look'), 2), ((20, 'mail'), 1), ((21, 'make'), 1), ((22, 'model'), 1), ((23, 'name'), 1), ((24, 'neighborhood'), 1), ((25, 'nntp_poste'), 1), ((26, 'organization'), 1), ((27, 'park'), 1), ((28, 'production'), 1), ((29, 'really'), 1), ((30, 'rest'), 1), ((31, 's'), 1), ((32, 'see'), 1), ((33, 'separate'), 1), ((34, 'small'), 1), ((35, 'spec'), 1), ((36, 'sport'), 1), ((37, 'tellme'), 1), ((38, 'thank'), 1), ((39, 'thing'), 1), ((40, 'umd'), 1), ((41, 'wonder'), 1), ((42, 'year'), 1)]]\n"
     ]
    }
   ],
   "source": [
    "#Create Dictionary (id2word), Dataset (texts), Document-Word frequency(corpus)\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View (Word_index, Word), Frequency\n",
    "print([[((id, id2word[id]), freq) for id, freq in cp] for cp in corpus[:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aacd79d-6322-4c88-ba21-4724361a38cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 11314\n",
      "Number of words: 50111\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents: ' + str(len(corpus)))\n",
    "print('Number of words: ' + str(len(id2word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb3e872f-7d8e-4e82-8536-c144b8d33c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 5), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 2), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f97a2e4-25f6-4450-9cb9-385e38633c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_documents = len(corpus)        \n",
    "num_tokens = len(id2word)  \n",
    "num_topics = 10               \n",
    "\n",
    "alpha = 1 / num_topics         \n",
    "beta = 1 / num_topics \n",
    "\n",
    "z_d_n = [[0 for _ in range(len(d))] for d in corpus]  # z_i_j\n",
    "theta_d_z = np.zeros((num_documents, num_topics))\n",
    "phi_z_w = np.zeros((num_topics, num_tokens))\n",
    "n_d = np.zeros((num_documents))\n",
    "n_z = np.zeros((num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0063d865-4b33-46ed-9b8c-15af8c93463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the parameters\n",
    "# m: doc id\n",
    "for d, doc in enumerate(corpus):  \n",
    "    # n: id of word inside document, w: id of the word globally\n",
    "    for n, w in enumerate(doc):\n",
    "        # assign a topic randomly to words\n",
    "        z_d_n[d][n] = n % num_topics\n",
    "        # get the topic for word n in document m\n",
    "        z = z_d_n[d][n]\n",
    "        # keep track of our counts\n",
    "        theta_d_z[d][z] += 1\n",
    "        phi_z_w[z, w] += 1\n",
    "        n_z[z] += 1\n",
    "        n_d[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a956d582-5222-42e0-87e3-70feb46cb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(10):\n",
    "    for d, doc in enumerate(corpus):\n",
    "        for n, w in enumerate(doc):\n",
    "            # get the topic for word n in document m\n",
    "            z = z_d_n[d][n]\n",
    "            w = w[0]\n",
    "\n",
    "            # decrement counts for word w with associated topic z\n",
    "            theta_d_z[d][z] -= 1\n",
    "            phi_z_w[z, w] -= 1\n",
    "            n_z[z] -= 1\n",
    "\n",
    "            # sample new topic from a multinomial according to our formular\n",
    "            p_d_t = (theta_d_z[d] + alpha) / (n_d[d] - 1 + num_topics * alpha)\n",
    "            p_t_w = (phi_z_w[:, w] + beta) / (n_z + num_tokens * beta)\n",
    "            p_z = p_d_t * p_t_w\n",
    "            p_z /= np.sum(p_z)\n",
    "            new_z = np.random.multinomial(1, p_z).argmax()\n",
    "\n",
    "            # set z as the new topic and increment counts\n",
    "            z_d_n[d][n] = new_z\n",
    "            theta_d_z[d][new_z] += 1\n",
    "            phi_z_w[new_z, w] += 1\n",
    "            n_z[new_z] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ee0ac80-6583-41e5-9779-ca202f13cac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33.  0.  6. ...  2.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0. 14. 42. ...  0.  7.  2.]\n",
      " ...\n",
      " [ 5.  0.  3. ...  0.  0.  4.]\n",
      " [ 0.  8.  3. ...  0. 17.  0.]\n",
      " [ 0.  0. 23. ...  0.  0.  0.]]\n",
      "----------\n",
      "[[1.1200e+02 7.1351e+04 1.1093e+04 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [7.0000e+00 7.0614e+04 1.0997e+04 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [1.0000e+01 6.9688e+04 1.0986e+04 ... 0.0000e+00 0.0000e+00 1.0000e+00]\n",
      " ...\n",
      " [2.7000e+01 6.4799e+04 1.0530e+04 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 6.3661e+04 1.0535e+04 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 6.2819e+04 1.0290e+04 ... 0.0000e+00 0.0000e+00 0.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(theta_d_z)\n",
    "print('----------')\n",
    "print(phi_z_w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
